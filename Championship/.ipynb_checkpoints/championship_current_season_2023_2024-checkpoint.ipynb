{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b1a92e",
   "metadata": {},
   "source": [
    "# Get League Table and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8ef5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "#imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e2aabe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8408/2833926469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Extract table data into a list of lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtable_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mrow_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"th\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"td\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtable_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# URL of the website\n",
    "url = \"https://fbref.com/en/comps/10/Championship-Stats\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "    # Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the table using its HTML class\n",
    "table = soup.find(\"table\", {\"class\": \"stats_table\"})\n",
    "\n",
    "    # Extract table data into a list of lists\n",
    "table_data = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    row_data = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "    table_data.append(row_data)\n",
    "\n",
    "    # Create a DataFrame from the list of lists\n",
    "columns = table_data[0]  # Assuming the first row contains column headers\n",
    "data = table_data[1:]\n",
    "league_table_df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb3c3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475585bc",
   "metadata": {},
   "source": [
    "# Get Fixtures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32813dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website\n",
    "url = \"https://fbref.com/en/comps/10/schedule/Championship-Scores-and-Fixtures\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the table containing fixtures\n",
    "    table = soup.find(\"table\", {\"class\": \"stats_table\"})\n",
    "\n",
    "    # Extract data into a list of dictionaries\n",
    "    fixtures_data = []\n",
    "    for row in table.find_all(\"tr\")[1:]:  # Skip the header row\n",
    "        columns = row.find_all([\"th\", \"td\"])\n",
    "        \n",
    "         # Extracting specific columns based on the structure of the table\n",
    "        fixture_type = columns[0].text.strip()\n",
    "        gameweek = columns[1].text.strip()\n",
    "        day_of_week = columns[2].text.strip()\n",
    "        date = columns[3].text.strip()\n",
    "        time = columns[4].text.strip()\n",
    "        home_team = columns[5].text.strip()\n",
    "        home_xG = columns[6].text.strip()\n",
    "        result = columns[7].text.strip()\n",
    "        away_xG = columns[8].text.strip()\n",
    "        away_team = columns[9].text.strip()\n",
    "        attendance = columns[10].text.strip()\n",
    "        stadium = columns[11].text.strip()\n",
    "        referee = columns[12].text.strip()\n",
    "\n",
    "        fixture_info = {\n",
    "            \"fixture_type\": fixture_type,\n",
    "            \"gameweek\": gameweek,\n",
    "            \"day_of_week\": day_of_week,\n",
    "            \"date\": date,\n",
    "            \"time\": time,\n",
    "            \"home_team\": home_team,\n",
    "            \"home_xG\": home_xG,\n",
    "            \"result\": result,\n",
    "            \"away_xG\": away_xG,\n",
    "            \"away_team\": away_team,\n",
    "            \"attendance\": attendance,\n",
    "            \"stadium\": stadium,\n",
    "            \"referee\": referee\n",
    "        }\n",
    "\n",
    "        fixtures_data.append(fixture_info)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    fixtures_table = pd.DataFrame(fixtures_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures_table['is_game_complete'] = np.where(len(fixtures_table['result']) > 0, 1, 0)\n",
    "#Checking to see if game is complete... if the result column is populated at all, it's been played"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471532f",
   "metadata": {},
   "source": [
    "# Getting Game IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website\n",
    "url = \"https://fbref.com/en/comps/10/schedule/Championship-Scores-and-Fixtures\"\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all <td> elements with data-stat=\"score\"\n",
    "score_cells = soup.find_all('td', {'data-stat': 'score'})\n",
    "\n",
    "# Extract game IDs from the <a> tags within the score_cells, the third element is the game ID\n",
    "game_ids = [cell.find('a')['href'].split('/')[3] for cell in score_cells if cell.find('a')]\n",
    "# Then strip the white space in the IDs\n",
    "game_ids = [x.strip(' ') for x in game_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = game_ids[0:4]\n",
    "#just use five to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1395c59",
   "metadata": {},
   "source": [
    "# Scraping Shot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "669a070c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for unique_id in game_ids:\n",
    "    # URL of the page to scrape\n",
    "    url = f'https://fbref.com/en/matches/{unique_id}/'\n",
    "\n",
    "    # Get the game ID from the URL\n",
    "    game_id = url.split(\"/\")[-2]\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the Shots table on the page\n",
    "    shots_table = soup.find(\"div\", {\"id\": \"switcher_shots\"})\n",
    "\n",
    "    # Create empty lists to store data and create the columns\n",
    "    data = []\n",
    "    headers = ['game_id', 'minute', 'player', 'team', 'xG', 'psxG', 'result', 'distance', 'body_part', 'notes', 'sca_1_player', 'event_1', 'sca_2_player', 'event_2']\n",
    "\n",
    "    # Extract table rows\n",
    "    rows = shots_table.find(\"tbody\").find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        row_data = [game_id] + [cell.get_text().strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Find the home and away teams and formations\n",
    "    team_headers = soup.find_all(\"th\", {\"colspan\": \"2\"})\n",
    "    teams = [header.text.strip().split(\" (\")[0] for header in team_headers if \"(\" in header.text.strip()]\n",
    "    formations = [header.text.strip().split(\" (\")[1][:-1] for header in team_headers if \"(\" in header.text.strip()]\n",
    "\n",
    "    # Extracting the home and away teams and formations\n",
    "    home_team = teams[0]\n",
    "    away_team = teams[1]\n",
    "    home_formation = formations[0]\n",
    "    away_formation = formations[1]\n",
    "\n",
    "    # Find the location and match date\n",
    "    location = soup.find(\"div\", {\"class\": \"scorebox_meta\"}).find_all(\"small\")[3].text\n",
    "    match_date = soup.find(\"span\", class_=\"venuetime\").get(\"data-venue-date\", None)\n",
    "\n",
    "    # Append additional columns to headers\n",
    "    headers.extend(['home_team', 'away_team', 'home_formation', 'away_formation', 'location', 'match_date'])\n",
    "\n",
    "    # Append data to all_data with additional columns\n",
    "    for row in data:\n",
    "        row.extend([home_team, away_team, home_formation, away_formation, location, match_date])\n",
    "        all_data.append(row)\n",
    "        \n",
    "        \n",
    "    time.sleep(4)\n",
    "        \n",
    "# Create a DataFrame from the scraped data\n",
    "shots_df = pd.DataFrame(all_data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1995eb33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'home_team'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'home_team'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4152/3403591390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add Home and away shots, location, hard-code competition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_home_shot'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'team'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'home_team'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_away_shot'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'team'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m  \u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'away_team'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mshots_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'competition'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'English Championship'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'home_team'"
     ]
    }
   ],
   "source": [
    "# Add Home and away shots, location, hard-code competition\n",
    "shots_df['is_home_shot'] = np.where(shots_df['team'] == shots_df['home_team'], 1, 0)\n",
    "shots_df['is_away_shot'] = np.where(shots_df['team'] ==  shots_df['away_team'], 1, 0)\n",
    "shots_df['competition'] = 'English Championship'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a93f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate total score for each game\n",
    "def calculate_total_score(group, shot):\n",
    "    total_score = 0\n",
    "    total_scores = []\n",
    "\n",
    "    # Flag to indicate whether the current row is after a 'Goal'\n",
    "    after_goal = False\n",
    "\n",
    "    for idx, (result, is_shot) in enumerate(zip(group, shot)):\n",
    "        # Check if the current row is after a 'Goal'\n",
    "        if after_goal:\n",
    "            # Increment the total score\n",
    "            total_score += 1\n",
    "            # Reset the flag\n",
    "            after_goal = False\n",
    "\n",
    "        # Check if the current result is 'Goal' and the corresponding shot is 1\n",
    "        if result == 'Goal' and is_shot == 1:\n",
    "            # Set the flag to True for the next iteration\n",
    "            after_goal = True\n",
    "        \n",
    "        # Append the current total score\n",
    "        total_scores.append(total_score)\n",
    "\n",
    "    return total_scores\n",
    "\n",
    "# Group by game_id and apply the function to calculate total score for home team\n",
    "shots_df['home_score'] = shots_df.groupby('game_id')['result'].transform(lambda x: calculate_total_score(x, shots_df['is_home_shot']))\n",
    "\n",
    "# Group by game_id and apply the function to calculate total score for away team\n",
    "shots_df['away_score'] = shots_df.groupby('game_id')['result'].transform(lambda x: calculate_total_score(x, shots_df['is_away_shot']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a nil nil column using numpy where and logical_and operators\n",
    "shots_df['is_nil_nil'] = np.where(np.logical_and(shots_df['home_score'] == 0, shots_df['away_score'] == 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding opposing team -- Whenever it's a home shot we want the away team, otherwise (away shot) we want the home team\n",
    "shots_df['opposing_team'] = np.where(shots_df['is_home_shot'] == 1, shots_df['away_team'], shots_df['home_team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4306cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace blank values with nulls\n",
    "shots_df['xG'] = shots_df['xG'].replace('', np.nan)\n",
    "\n",
    "#Then drop them\n",
    "shots_df = shots_df.dropna(subset=['xG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df['xG'] = shots_df['xG'].astype('float64') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c6ed6",
   "metadata": {},
   "source": [
    "# Adding xG and xGA while nil-nil to league_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef122ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter shots_df where is_nil_nil is 0\n",
    "filtered_shots_df = shots_df[shots_df['is_nil_nil'] == 1]\n",
    "\n",
    "# Calculate the sum of 'xG' for each 'team'\n",
    "team_xG_sum = filtered_shots_df.groupby('team')['xG'].sum().reset_index()\n",
    "\n",
    "# Create a dictionary to map team names to their respective summed 'xG' values\n",
    "team_xG_dict = dict(zip(team_xG_sum['team'], team_xG_sum['xG']))\n",
    "\n",
    "# Add 'nil_nil_xG' column to league_table_df by mapping team names to their summed 'xG' values\n",
    "league_table_df['nil_nil_xG'] = league_table_df['Squad'].map(team_xG_dict)\n",
    "\n",
    "# Fill NaN values with 0 if any team didn't have any 'xG' when is_nil_nil is 0\n",
    "league_table_df['nil_nil_xG'].fillna(0, inplace=True)\n",
    "\n",
    "# xGA\n",
    "\n",
    "# Calculate the sum of 'xGA' for each 'team'\n",
    "team_xGA_sum = filtered_shots_df.groupby('opposing_team')['xG'].sum().reset_index()\n",
    "\n",
    "# Create a dictionary to map team names to their respective summed 'xGA' values -- AKA xG of opposing_team\n",
    "team_xGA_dict = dict(zip(team_xGA_sum['opposing_team'], team_xGA_sum['xG']))\n",
    "\n",
    "# Add 'nil_nil_xGA' column to league_table_df by mapping team names to their summed 'xGA' values\n",
    "league_table_df['nil_nil_xGA'] = league_table_df['Squad'].map(team_xGA_dict)\n",
    "\n",
    "# Fill NaN values with 0 if any team didn't have any 'xG' when is_nil_nil is 0\n",
    "league_table_df['nil_nil_xGA'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ffadb",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d678d",
   "metadata": {},
   "source": [
    "1. Add average xG and xGA per match while nil nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e5604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
